{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning in Action\n",
    "## Course1: Introduction to Reinforcement Learning\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course Syllabus\n",
    "---\n",
    "- What is RL?\n",
    "- What kind of problems does RL solve?\n",
    "- **C1: Foolsball**\n",
    "- Modeling problems using the RL framework: Agent, Environment, State, Goals, Rewards, Returns\n",
    "- MDPs and single-step dynamics\n",
    "- State-values, action values, policies, optimality\n",
    "- Solving MDP with known single-step dynamics\n",
    "- Monte Carlo estimation, greedy policies, exploration and exploitation, epsilon-greedy policies\n",
    "- TD methods: Sarsa, Sarsamax Q-learning, Expected Sarsa\n",
    "- **P1: Taxi-V3**\n",
    "- Discretization: Tile-coding, Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 4 Summary\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding Uncertainty To The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Introduced Uncertainty Through Bad Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Action exectuted by environment could be different from the action selected by the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now have to deal with a distribution of possible next states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning to Deal with Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our methods are based on sampling so they should be able to detect multiple possible outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making The Environment Dynamic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Opponents can move in their columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Agents needs to learn to sense agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduce Incomplete State Observations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Agent can only sense 4 neighboring positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The presence of opponents along with the agent's own position makes up the observed state now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Encoding the observed state efficiently is the agent's responsibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The number of observed states needs to be kept small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Readings\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- TD($\\lambda$) updates [\\[1\\]](https://www.jeremyjordan.me/rl-learning-methods/) [\\[2\\]](https://amreis.github.io/ml/reinf-learn/2017/11/02/reinforcement-learning-eligibility-traces.html)\n",
    "\n",
    "- Discretization [\\[1\\]](http://incompleteideas.net/book/8/node5.html) [\\[2\\]](http://incompleteideas.net/book/8/node5.html) [\\[3\\]](http://incompleteideas.net/book/8/node7.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Environment and Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Many problems requiring **learning** to plan and control can be modeled as an agent and environment set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTMDmrmnl_dAyjCOErHPak2gLXmQTgQnVT8gQ&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The agent performs an action in the environment\n",
    "- The state of the environment and agent change as a result\n",
    "- The agent receives a reward and the updated state from the environment\n",
    "- The agent and environment distinction is a little tricky to define "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sutton & Barto**\n",
    "> The agentâ€“environment boundary can be located at different places for different purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### State and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The state is a description of the environment and the agent, that is used for decision making\n",
    "- The decisions that the agent makes are its actions and they impact the state of the system \n",
    "- Technically state is the precise desciption of the system while observations are incomplete/noisy state information [\\[1\\]](https://ai.stackexchange.com/a/5971/23273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Markov Decision Processes (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A markov decision process is a state transition model [\\[1\\]](https://setosa.io/ev/markov-chains/) that takes an action in every state and produces a reward along with any state transition.\n",
    "- They're Markovian because the transition probabilities in any state only depend on the current state and not on the previous states that led to the current state.\n",
    "\n",
    "- An MDP is made up of the following\n",
    "    - A set $S$ representing all the states\n",
    "    - A set $A$ representing all possible actions \n",
    "    - A transition matrix P that maps (state,action) pairs to next states: $P:S \\times A \\rightarrow S$\n",
    "    - A reward matirx R that maps (state,next state) pairs to immediate rewards(real numbers): $R:S \\times S \\rightarrow \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When the environment is stochastic there could be multiple possible next states for every state action pair. The transition table is then three dimensional \n",
    "$$P: S \\times A \\times S \\rightarrow [0,1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Single-Step Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The transition annd rewards tables are collectively known as the single-step dynamics\n",
    "- The dynamics are generally not known to the agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rewards, Returns and Discounted Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The immediate reward that follows a state transition is denoted by $R$.\n",
    "- The cumulative reward from a state to a terminal state is denoted by $G$ and called return.\n",
    "- The cumulative reward is often discounted when training an agent to nudge the agent from looking too far in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A deterministic policy maps every state to an action\n",
    "$$ \\pi : S \\rightarrow A$$\n",
    "\n",
    "- An RL algorithm's goal is to find an optimal policy $\\pi^*$ that maps every state to the action with the highest discounted return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### State Value and Action Value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The state value function maps each state to the estimated discounted return for that state when following a policy $\\pi$\n",
    "$$V_{\\pi}: S \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "- The action value function maps each $(state, action)$ pair to the estimated discounted return correspoding to the pair when following a policy $\\pi$\n",
    "$$Q_{\\pi}: S \\times A \\rightarrow \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Monte Carlo Estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Monte Carlo methods are algorithms that use random sampling to find numerical solutions to problems [\\[1\\]](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/monte-carlo-methods), [\\[2\\]](https://brilliant.org/wiki/monte-carlo/).\n",
    "\n",
    "- Monte Carlo estimation finds the single step dynamics by sampling the environment through interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First Visit vs Every Visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When calculating returns for a $(state,action)$ pair from rewards estimated using Monte Carlo sampling\n",
    "    - First visit estimation considers all visits to the $(state,action)$ pair in an episode.\n",
    "    - Every visit estimation considers all visits to the $(state,action)$ pair in an episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Greedy Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A greedy policy selects the optimal action (action with the highest discounted return) in every state\n",
    "- This requires use of the action value function $Q_{\\pi}$\n",
    "- Since action values are discounted returns and thus include future rewards,the greedily-selected actions are optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $\\epsilon$-Greedy Policies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An epsilon greedy policy selects the optimal action with a high probability but can also select a non-optimal action with a small probability \n",
    "\n",
    "- The hyper-paramtere $\\epsilon$ controls the probability of selecting a non-optimal action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimal State and Action Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An optimal action value function $Q^*_{\\pi}$ is the maximum action value function over all possible policies.\n",
    "- An optimal state value function $V^*_{\\pi}$ is the maximum state value function over all possible policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An optimal policy $\\pi^*$ is a (greedy) policy derived from an optimal action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Policy evaluation is the process of using a policy to estimate the state value function $V_{\\pi}$corresponding to that policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Policy improvement is the process of updating the action value function $Q_{\\pi}$ and ultimately the policy $\\pi$ based on the state value function $V_{\\pi}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bellman (Expectation) Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$Q_{\\pi}(s_0,a_0) = R(s_0,a_0) + \\gamma * \\sum\\limits_{s_1 \\in S} P(s_1 | s_0,a_0) * V_{\\pi}(s_1)$, where\n",
    "\n",
    "$V_{\\pi}(s_1) = \\sum\\limits_{a_1\\in A} \\pi(a_1|s_1)*Q_{\\pi}(s_1,a_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Epsilon Decay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

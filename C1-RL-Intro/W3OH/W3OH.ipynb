{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning in Action\n",
    "## Course1: Introduction to Reinforcement Learning\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course Syllabus\n",
    "---\n",
    "- What is RL?\n",
    "- What kind of problems does RL solve?\n",
    "- **C1: Foolsball**\n",
    "- Modeling problems using the RL framework: Agent, Environment, State, Goals, Rewards, Returns\n",
    "- MDPs and single-step dynamics\n",
    "- State-values, action values, policies, optimality\n",
    "- Solving MDP with known single-step dynamics\n",
    "- Monte Carlo estimation, greedy policies, exploration and exploitation, epsilon-greedy policies\n",
    "- TD methods: Sarsa, Sarsamax Q-learning, Expected Sarsa\n",
    "- **P1: Taxi-V3**\n",
    "- Discretization: Tile-coding, Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 3 Summary\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The exploration-exploitation dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploration using random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Select a uniformly random action in every state within an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Good strategy to learn about the environment in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "action = np.random.choice(foolsball.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploitation using a greedy sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Select the action corresponding to the highest discounted return in every state within an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Does not give good results in the beginning when the returns table is based on very little experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "greedy_action = table.loc[state].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combining Exploration and Exploitation  with $\\epsilon$-greedy sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Select the action with the highest discounted return most of the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Select a random action with a small probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "actions = table.columns\n",
    "action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
    "\n",
    "greedy_action_index = np.argmax(table.loc[state].values)\n",
    "action_probs[greedy_action_index] += 1-epsilon\n",
    "\n",
    "epsilon_greedy_action = np.random.choice(table.columns,p=action_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $\\epsilon$ Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Start with pure exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Rely more and more on exploitation with each passing episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns,epsilon=epsilon)\n",
    "epsilon *= epsilon_decay\n",
    "epsilon = max(epsilon, min_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Converging Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Constant Alpha ($\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The division by number of visits has a non-linear effect on the size of every new update to the returns table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Use the difference of existing estimate and new estimate based on an episode to update the existing estimate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ESTIMATED_RETURNS_TBL.loc[s,a] += alpha*(ret - ESTIMATED_RETURNS_TBL.loc[s,a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Temporal Difference (TD) Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Update the returns table after every step of an episode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the absence of an entire episode, use the difference of the current estimate and new estimate based on just on step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ESTIMATED_RETURNS_TBL.loc[s0,a0] +=\\\n",
    "    alpha*(reward + HYPER_PARAMS['gamma']*ESTIMATED_RETURNS_TBL.loc[s1,a1] - ESTIMATED_RETURNS_TBL.loc[s0,a0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "for i in tqdm(range(n_episodes)):\n",
    "    foolsball.reset()\n",
    "    s0 = foolsball.init_state\n",
    "    a0 = epsilon_greedy_action_from_Q(Q,s0,epsilon)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        s1, reward, done  = foolsball.step(a0)\n",
    "        a1 = epsilon_greedy_action_from_Q(Q,s1,epsilon)\n",
    "        \n",
    "        Q.loc[s0,a0] += alpha*(reward + HYPER_PARAMS['gamma']*Q.loc[s1,a1] - Q.loc[s0,a0])\n",
    "        \n",
    "        s0, a0 = s1, a1\n",
    "  \n",
    "    epsilon *= epsilon_decay\n",
    "    epsilon = max(epsilon,min_epsilon)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "for i in tqdm(range(n_episodes)):\n",
    "    foolsball.reset()\n",
    "    s0 = foolsball.init_state\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        a0 = epsilon_greedy_action_from_Q(Q,s0,epsilon)\n",
    "        s1, reward, done  = foolsball.step(a0)\n",
    "        \n",
    "        Q.loc[s0,a0] += alpha*(reward + HYPER_PARAMS['gamma']*Q.loc[s1].max() - Q.loc[s0,a0])\n",
    "        \n",
    "        s0 = s1\n",
    "  \n",
    "    epsilon *= epsilon_decay\n",
    "    epsilon = max(epsilon,min_epsilon)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Environment and Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Many problems requiring **learning** to plan and control can be modeled as an agent and environment set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTMDmrmnl_dAyjCOErHPak2gLXmQTgQnVT8gQ&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The agent performs an action in the environment\n",
    "- The state of the environment and agent change as a result\n",
    "- The agent receives a reward and the updated state from the environment\n",
    "- The agent and environment distinction is a little tricky to define "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sutton & Barto**\n",
    "> The agent–environment boundary can be located at different places for different purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### State and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The state is a description of the environment and the agent, that is used for decision making\n",
    "- The decisions that the agent makes are its actions and they impact the state of the system \n",
    "- Technically state is the precise desciption of the system while observations are incomplete/noisy state information [\\[1\\]](https://ai.stackexchange.com/a/5971/23273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Markov Decision Processes (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A markov decision process is a state transition model [\\[1\\]](https://setosa.io/ev/markov-chains/) that takes an action in every state and produces a reward along with any state transition.\n",
    "- They're Markovian because the transition probabilities in any state only depend on the current state and not on the previous states that led to the current state.\n",
    "\n",
    "- An MDP is made up of the following\n",
    "    - A set $S$ representing all the states\n",
    "    - A set $A$ representing all possible actions \n",
    "    - A transition matrix P that maps (state,action) pairs to next states: $P:S \\times A \\rightarrow S$\n",
    "    - A reward matirx R that maps (state,next state) pairs to immediate rewards(real numbers): $R:S \\times S \\rightarrow \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When the environment is stochastic there could be multiple possible next states for every state action pair. The transition table is then three dimensional \n",
    "$$P: S \\times A \\times S \\rightarrow [0,1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Single-Step Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The transition annd rewards tables are collectively known as the single-step dynamics\n",
    "- The dynamics are generally not known to the agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rewards, Returns and Discounted Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The immediate reward that follows a state transition is denoted by $R$.\n",
    "- The cumulative reward from a state to a terminal state is denoted by $G$ and called return.\n",
    "- The cumulative reward is often discounted when training an agent to nudge the agent from looking too far in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A deterministic policy maps every state to an action\n",
    "$$ \\pi : S \\rightarrow A$$\n",
    "\n",
    "- An RL algorithm's goal is to find an optimal policy $\\pi^*$ that maps every state to the action with the highest discounted return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### State Value and Action Value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The state value function maps each state to the estimated discounted return for that state when following a policy $\\pi$\n",
    "$$V_{\\pi}: S \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "- The action value function maps each $(state, action)$ pair to the estimated discounted return correspoding to the pair when following a policy $\\pi$\n",
    "$$Q_{\\pi}: S \\times A \\rightarrow \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Monte Carlo Estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Monte Carlo methods are algorithms that use random sampling to find numerical solutions to problems [\\[1\\]](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/monte-carlo-methods), [\\[2\\]](https://brilliant.org/wiki/monte-carlo/).\n",
    "\n",
    "- Monte Carlo estimation finds the single step dynamics by sampling the environment through interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First Visit vs Every Visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When calculating returns for a $(state,action)$ pair from rewards estimated using Monte Carlo sampling\n",
    "    - First visit estimation considers all visits to the $(state,action)$ pair in an episode.\n",
    "    - Every visit estimation considers all visits to the $(state,action)$ pair in an episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Greedy Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A greedy policy selects the optimal action (action with the highest discounted return) in every state\n",
    "- This requires use of the action value function $Q_{\\pi}$\n",
    "- Since action values are discounted returns and thus include future rewards,the greedily-selected actions are optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $\\epsilon$-Greedy Policies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An epsilon greedy policy selects the optimal action with a high probability but can also select a non-optimal action with a small probability \n",
    "\n",
    "- The hyper-paramtere $\\epsilon$ controls the probability of selecting a non-optimal action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimal State and Action Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An optimal action value function $Q^*_{\\pi}$ is the maximum action value function over all possible policies.\n",
    "- An optimal state value function $V^*_{\\pi}$ is the maximum state value function over all possible policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An optimal policy $\\pi^*$ is a (greedy) policy derived from an optimal action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Policy evaluation is the process of using a policy to estimate the state value function $V_{\\pi}$corresponding to that policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Policy improvement is the process of updating the action value function $Q_{\\pi}$ and ultimately the policy $\\pi$ based on the state value function $V_{\\pi}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "TBD after introduction to stochastic policies and environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Epsilon Decay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
